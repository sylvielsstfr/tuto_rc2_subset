pipelineYaml: '$DRP_PIPE_DIR/pipelines/HSC/DRP-RC2_subset.yaml'

# Just names, but UPDATE THIS AND KEEP THEM SHORT
project: hsc_drp
campaign: HSCDRP2024

# UPDATE THESE arguments to mirror the ones you use with command-line pipetask run
payload:
  payloadName: 'u/dagoret/single_frame'
  butlerConfig: '$RC2_SUBSET_DIR/SMALL_HSC/butler.yaml'
  inCollection: LATISS/raw/all,refcats,LATISS/calib
  dataQuery: "instrument='LATISS' AND exposure.day_obs=20230912 AND physical_filter='collimator~holo4_003' AND exposure.observation_type='science' AND exposure.science_program='SITCOM-1001'"
  runPreCmdOpts: --long-log --log-level timer.lsst.resources=DEBUG 

wmsServiceClass: lsst.ctrl.bps.parsl.ParslService
computeSite: slurm

parsl:
  log_level: INFO

site:
  slurm:
    class: lsst.ctrl.bps.parsl.sites.Slurm
    nodes: 2
    walltime: '24:00:00'     # This is 20 hours
    cores_per_node: 100
    qos: normal
    scheduler_options: |
      #SBATCH --partition=roma
      #SBATCH --exclusive
  triple_slurm:
    class: lsst.ctrl.bps.parsl.sites.TripleSlurm
    nodes: 1
    cores_per_node: 100
    qos: normal
    small_memory: 2.0     # Units are GB
    medium_memory: 4.0
    large_memory: 8.0
    small_walltime: '10.0'   # Units are hours
    medium_walltime: '10.0'
    large_walltime: '40.0'        
  work_queue:
    class: lsst.ctrl.bps.parsl.sites.work_queue.LocalSrunWorkQueue
    worker_options: "--memory=480000"   # work_queue expects memory in MB
    nodes_per_block: 10

    monitorEnable: true
    monitorFilename: monitoring.db

